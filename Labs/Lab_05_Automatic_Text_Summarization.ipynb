{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_05_Automatic_Text_Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescodisalvo05/66DaysOfData/blob/main/Labs/Lab_05_Automatic_Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23KZw7Xd5XhK"
      },
      "source": [
        "#**Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teaching Assistant:** Moreno La Quatra\n",
        "\n",
        "**Practice 5:** Automatic Text Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlb9Te9I5exv"
      },
      "source": [
        "## Extractive Text Summarization\n",
        "\n",
        "Content is extracted from the original data, but the extracted content is not modified in any way.\n",
        "\n",
        "![](https://images.deepai.org/machine-learning-models/8f66b1eb608e4eb681b2ec0c0631385c/summarization.jpg)\n",
        "\n",
        "For this part of the practice we will use the BBC News Summary dataset available in [Kaggle](https://www.kaggle.com/pariza/bbc-news-summary)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiCTbAwGA6RK"
      },
      "source": [
        "%%capture\n",
        "! wget https://github.com/MorenoLaQuatra/DeepNLP/raw/main/practices/P5/bbc_news.zip\n",
        "! unzip bbc_news.zip"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoJgwbjDFCAq"
      },
      "source": [
        "### **Question 1: split data collection**\n",
        "\n",
        "Read the data collection and split it into train/test/eval. Data are provided with different classes (e.g., business, sport, tech...), be sure to select 10% of data for testing **for each class**.\n",
        "\n",
        "**Note 1:** Some files can report UnicodeError, feel free to ignore it (`errors` parameter)\n",
        "\n",
        "**Note 2:** you can fix encoding after file reading by using [ftfy](https://pypi.org/project/ftfy/) library "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFw-VeeLJCS7"
      },
      "source": [
        "!pip install ftfy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r_evLR5FBq4"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWGRHPMJucXY"
      },
      "source": [
        "import os \n",
        "import pandas as pd\n",
        "import ftfy\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2cCtNPRqr7R"
      },
      "source": [
        "classes = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
        "path = '/content/BBC News Summary/'\n",
        "\n",
        "dataset = {}\n",
        "\n",
        "for c in classes:\n",
        "\n",
        "  dataset[c] = {}\n",
        "\n",
        "  articles = os.listdir(path + \"/News Articles/\" + c )\n",
        "\n",
        "  # the name is the same for the summary\n",
        "  for article in articles:\n",
        "\n",
        "    # article : 001.txt (id = 001)\n",
        "    id = article.split(\".\")[0]\n",
        "\n",
        "    dataset[c][id] = {}\n",
        "\n",
        "    # load article\n",
        "    with open(path + \"News Articles/\" + c + \"/\" + article, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "      curr_data = f.readlines()\n",
        "\n",
        "      # clean rows\n",
        "\n",
        "      phrases = []\n",
        "\n",
        "      for line in curr_data:\n",
        "        if line != \"\\n\": # remove \\n rows\n",
        "          curr_line = line.replace(\"\\n\",\"\") # remove final \\n\n",
        "          phrases.append(ftfy.fix_text(curr_line)) # fix encoding\n",
        "\n",
        "      \n",
        "      dataset[c][id][\"text\"] = ' '.join(phrases)\n",
        "\n",
        "      f.close()\n",
        "\n",
        "    # load summary\n",
        "    with open(path + \"Summaries/\" + c + \"/\" + article, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "      curr_data = f.readlines()\n",
        "\n",
        "      # clean rows\n",
        "\n",
        "      phrases = []\n",
        "\n",
        "      for line in curr_data:\n",
        "        if line != \"\\n\": # remove \\n rows\n",
        "          curr_line = line.replace(\"\\n\",\"\") # remove final \\n\n",
        "          phrases.append(ftfy.fix_text(curr_line)) # fix encoding\n",
        "\n",
        "      dataset[c][id][\"summary\"] = ' '.join(phrases)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gS-puWEubLk"
      },
      "source": [
        "# taken inspiration from instructor's solution\n",
        "\n",
        "train_ds, eval_ds, test_ds = [], [], []\n",
        "\n",
        "for c in classes:\n",
        "\n",
        "  # get the keys for each class\n",
        "  keys_per_class = list(dataset[c].keys())\n",
        "\n",
        "  train, eval_test = train_test_split(keys_per_class, test_size=0.2)\n",
        "  test, eval = train_test_split(eval_test, test_size=0.5)\n",
        "\n",
        "  for k in train:\n",
        "    train_ds.append(dataset[c][k])\n",
        "\n",
        "  for k in eval:\n",
        "    eval_ds.append(dataset[c][k])\n",
        "\n",
        "  for k in test:\n",
        "    test_ds.append(dataset[c][k])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTHNVAqn1Pfg"
      },
      "source": [
        "### **Question 2: Unsupervised Text Summarization (TextRank)**\n",
        "\n",
        "[TextRank](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf) is an unsupervised text summarization approach that relies on graph modelling. Implement a `TextrankSummarizer` class that expose the `summarize(sentences, N)` function to get the `N` most relevant sentences from a list (`sentences`). \n",
        "\n",
        "The main steps are reported here:\n",
        "\n",
        "1. Each sentence is a node in a graph (undirected)\n",
        "2. A pair of sentence is connected with an edge whose weight is computed according to the number of common words (see Note 1).\n",
        "3. Pagerank is used to compute a relevance score for each node in the graph (for each sentence in the list)\n",
        "4. The `summarize` function return the summary concatenating the `N`  most relevant sentences (according to the score computed at step 3).\n",
        "\n",
        "**Note 1:** An example of the similarity function that can be used to compute graph weights is repoted below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDzCPn0E41sJ"
      },
      "source": [
        "import math\n",
        "\n",
        "def compute_similarity(tokens_sent_1, tokens_sent_2):\n",
        "\n",
        "    n_common_words = len(set(tokens_sent_1) & set(tokens_sent_2))\n",
        "\n",
        "    log_s1 = math.log10(len(tokens_sent_1))\n",
        "    log_s2 = math.log10(len(tokens_sent_2))\n",
        "\n",
        "    if log_s1 + log_s2 == 0:\n",
        "        return 0\n",
        "\n",
        "    return n_common_words / (log_s1 + log_s2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPD6t1G1mWKG"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1sxerzWxiDv"
      },
      "source": [
        "import networkx as nx\n",
        "\n",
        "class TextrankSummarizer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nodes = set()\n",
        "        self.edges = []\n",
        "        self.graph = nx.DiGraph()\n",
        "\n",
        "   \n",
        "    def summarize(self, sentences, N=2):\n",
        "\n",
        "      len_sentences = len(sentences)\n",
        "\n",
        "      for i in range(len_sentences):\n",
        "        for j in range(i,len_sentences):\n",
        "          \n",
        "          self.nodes.add(sentences[i])\n",
        "          self.nodes.add(sentences[j])\n",
        "\n",
        "          # add weighted edges to the graph\n",
        "          weight = compute_similarity(sentences[i], sentences[j])\n",
        "          self.edges.append((sentences[i], sentences[j], weight))\n",
        "          self.edges.append((sentences[j], sentences[i], weight))\n",
        "\n",
        "      # page rank\n",
        "      self.graph.add_nodes_from(list(self.nodes))\n",
        "      self.graph.add_weighted_edges_from(self.edges)\n",
        "\n",
        "      p = nx.pagerank(self.graph, max_iter=100)\n",
        "\n",
        "      # sort the results in descending order and take the top N\n",
        "      ordered_scores = sorted(list(p.items()), key=lambda x : -x[1])\n",
        "      top_N_tuple = sorted(list(p.items()), key=lambda x : -x[1])[:N]\n",
        "      top_N = [t[0] for t in top_N_tuple]\n",
        "\n",
        "      return top_N"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wYBaG7g7Jfo"
      },
      "source": [
        "### **Question 3: Unsupervised Text Summarization (TextRank + TF-IDF)**\n",
        "\n",
        "Implement a `TextrankTFIDFSummarizer` class that expose the `summarize(sentences, N)` function to get the `N` most relevant sentences from a list (`sentences`). \n",
        "\n",
        "Implement the class similarly to Q2. This version uses a different similarity function to weigh edges connecting sentences. It uses [TF-IDF vectorization](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) and [cosine similarity](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) to compute sentence-to-sentence similarity.\n",
        "\n",
        "- Compute TF-IDF vectors for each sentence\n",
        "- Compute edges' weights using the cosine similarity between TF-IDF vector representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uX1XCEVmarN"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxhVLN7ZNk-4"
      },
      "source": [
        "import networkx as nx\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxIxo2cYNiRG"
      },
      "source": [
        "def compute_similarity_tfidf(tokens_sent_1, tokens_sent_2):\n",
        "        \n",
        "  vectorizer = TfidfVectorizer()\n",
        "\n",
        "  tfidf_1 = vectorizer.fit_transform([tokens_sent_1])\n",
        "\n",
        "  # do only transform for having the same shape\n",
        "  tfidf_2 = vectorizer.transform([tokens_sent_2])\n",
        "\n",
        "  return cosine_similarity(tfidf_1,tfidf_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18xm5SUuAS_v"
      },
      "source": [
        "class TextrankTFIDFSummarizer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nodes = set()\n",
        "        self.edges = []\n",
        "        self.graph = nx.DiGraph()\n",
        "\n",
        "   \n",
        "    def summarize(self, sentences, N=2):\n",
        "\n",
        "      len_sentences = len(sentences)\n",
        "\n",
        "      for i in range(len_sentences):\n",
        "        for j in range(len_sentences):\n",
        "\n",
        "          if i == j:\n",
        "            continue\n",
        "\n",
        "          self.nodes.add(sentences[i])\n",
        "          self.nodes.add(sentences[j])\n",
        "\n",
        "          # add weighted edges to the graph\n",
        "          weight = compute_similarity_tfidf(sentences[i], sentences[j])\n",
        "          self.edges.append((sentences[i], sentences[j], weight))\n",
        "\n",
        "      # page rank\n",
        "      self.graph.add_nodes_from(list(self.nodes))\n",
        "      self.graph.add_weighted_edges_from(self.edges)\n",
        "\n",
        "      p = nx.pagerank(self.graph, max_iter=100)\n",
        "\n",
        "      # sort the results in descending order and take the top N\n",
        "      ordered_scores = sorted(list(p.items()), key=lambda x : -x[1])\n",
        "      top_N_tuple = sorted(list(p.items()), key=lambda x : -x[1])[:N]\n",
        "      top_N = [t[0] for t in top_N_tuple]\n",
        "\n",
        "      return top_N"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVRrBOKxCPyR"
      },
      "source": [
        "### **Question 4: Unsupervised Text Summarization (Pretrained BERT)**\n",
        "\n",
        "Both Textrank and Lexrank relies on syntactic scores to compute sentence similarity. \n",
        "Use Sentence-Transformer library to encode sentences into semantic-aware vectors and compute semantic similarity to interconnect sentences (e.g., use cosine similarity of bert encodings). Implement `BERTSummarizer` class similarly to Q2 and Q3.\n",
        "\n",
        "Note 1: use `sentence-transformers` library to obtain sentence embeddings (https://www.sbert.net/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzCUdyrkmchL"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYPJKTM1CcEE"
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9E__nDECPHp"
      },
      "source": [
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "class  BERTSummarizer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nodes = set()\n",
        "        self.edges = []\n",
        "        self.graph = nx.DiGraph()\n",
        "        self.bert = SentenceTransformer(\"stsb-mpnet-base-v2\")\n",
        "\n",
        "    \n",
        "    def compute_similarity(self, tokens_sent_1, tokens_sent_2):\n",
        "        \n",
        "        tokens_enc_1 = self.bert.encode(tokens_sent_1)\n",
        "        tokens_enc_2 = self.bert.encode(tokens_sent_2)\n",
        "        \n",
        "        return cosine_similarity(tokens_enc_1.reshape(-1,1),tokens_enc_2.reshape(-1,1))\n",
        "\n",
        "   \n",
        "    def summarize(self, sentences, N=2):\n",
        "\n",
        "      len_sentences = len(sentences)\n",
        "\n",
        "      for i in range(len_sentences):\n",
        "        for j in range(i,len_sentences):\n",
        "\n",
        "          self.nodes.add(sentences[i])\n",
        "          self.nodes.add(sentences[j])\n",
        "\n",
        "          # add weighted edges to the graph\n",
        "          weight = self.compute_similarity(sentences[i], sentences[j])\n",
        "          self.edges.append((sentences[i], sentences[j], weight))\n",
        "          self.edges.append((sentences[j], sentences[i], weight))\n",
        "\n",
        "      # page rank\n",
        "      self.graph.add_nodes_from(list(self.nodes))\n",
        "      self.graph.add_weighted_edges_from(self.edges)\n",
        "\n",
        "      p = nx.pagerank(self.graph, max_iter=100)\n",
        "\n",
        "      # sort the results in descending order and take the top N\n",
        "      ordered_scores = sorted(list(p.items()), key=lambda x : -x[1])\n",
        "      top_N_tuple = sorted(list(p.items()), key=lambda x : -x[1])[:N]\n",
        "      top_N = [t[0] for t in top_N_tuple]\n",
        "\n",
        "      return top_N"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmSnChfIFSKc"
      },
      "source": [
        "### **Question 5: ROUGE-based evaluation**\n",
        "\n",
        "Using only the **test set** obtained in Q1 compare the performance of the three summarizers implemented in Q2, Q3 and Q4. \n",
        "\n",
        "Report their results in terms of average precision, recall and F1-score for Rouge 2 metrics. Set the number of extracted sentences to 4 for all summarizers.\n",
        "\n",
        "**Which method obtain the best scores?**\n",
        "\n",
        "Note 1: You can use the python implementation of ROUGE available [here](https://pypi.org/project/rouge/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW42pxUXEhSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b820e49-b63c-4b2b-9aab-9abd12659a53"
      },
      "source": [
        "! pip install rouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dba9zCcZmfpd"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC2wwjKYNU6g"
      },
      "source": [
        "textrank = TextrankSummarizer()\n",
        "summary_text_rank = textrank.summarize(X_test,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfajDcikObG8"
      },
      "source": [
        "textrank_tfidf = TextrankTFIDFSummarizer()\n",
        "summary_tr_tfidf = textrank_tfidf.summarize(X_test,4)\n",
        "\n",
        "# -- row, column, and data arrays must be 1-D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRnz10BQOibr"
      },
      "source": [
        "textrank_bert = BERTSummarizer()\n",
        "summary_bert = textrank_bert.summarize(X_test,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTHHcu0IIxeH"
      },
      "source": [
        "## Abstractive Text Summarization\n",
        "\n",
        "Abstractive methods build an internal semantic representation of the original content, and then use this representation to create a summary that is closer to what a human might express. Abstraction may transform the extracted content by paraphrasing sections of the source document, to condense a text more strongly than extraction. Such transformation, however, is computationally much more challenging than extraction.\n",
        "\n",
        "![https://techcommunity.microsoft.com/t5/image/serverpage/image-id/180981i9EA877DDFF97D50D?v=v2](https://techcommunity.microsoft.com/t5/image/serverpage/image-id/180981i9EA877DDFF97D50D?v=v2)\n",
        "\n",
        "Also for this part of the practice we use the BBC News Summary dataset available in [Kaggle](https://www.kaggle.com/pariza/bbc-news-summary)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQbCIzgtKIkV"
      },
      "source": [
        "### **Question 6: BART (pretrained) seq2seq model**\n",
        "\n",
        "Exploit [BART](https://huggingface.co/facebook/bart-large-cnn) pretrained on CNN Daily Mail dataset to summarize the article in the BBC test set. Compute the obtained scores in terms of average precision, recall and F1-score for Rouge 2 metrics.\n",
        "\n",
        "Note 1: for generated summaries set the maximum length to 100 and the minimum length to your preferred value.\n",
        "\n",
        "Note 2: **to speed up computation**, you can use the distilled version of the BART model (e.g., `sshleifer/distilbart-cnn-12-6` available [here](https://huggingface.co/sshleifer/distilbart-cnn-12-6))\n",
        "\n",
        "Note 3: You can use the [summarization pipeline](https://huggingface.co/transformers/main_classes/pipelines.html#transformers.SummarizationPipeline). Explictly set truncation to True to avoid index errors (e.g. `summarizer(..., truncation=True)`)\n",
        "\n",
        "Note 4: Explictly set the device to use GPU acceleration (colab runtime should be also set to GPU) while creating the pipeline object (e.g., `pipeline(..., device=0)`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FXMx1i0mlnb"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpMINwqy67Bi",
        "outputId": "4fda3ebc-f9f0-417d-aa24-1b94c95001e1"
      },
      "source": [
        "!pip install Rouge"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from Rouge) (1.15.0)\n",
            "Installing collected packages: Rouge\n",
            "Successfully installed Rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x4qNDo-5scU"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPT213s2qS1N"
      },
      "source": [
        "from transformers import pipeline \n",
        "from rouge import Rouge\n",
        "\n",
        "# BART \n",
        "summarizer = pipeline(\"summarization\", \n",
        "                      model=\"sshleifer/distilbart-cnn-12-6\", \n",
        "                      tokenizer=\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "rouge = Rouge()\n",
        "\n",
        "precision, recall, f1_score, summaries, pred = [], [], [], [], []\n",
        "\n",
        "for v in test_ds:\n",
        "  \n",
        "  summary = v[\"summary\"]\n",
        "  \n",
        "  pred = summarizer(v[\"text\"], do_sample=False, truncation=True)\n",
        "  pred = pred[0][\"summary_text\"] \n",
        "  \n",
        "  scores = rouge.get_scores(pred, summary) \n",
        "  \n",
        "  precision.append(scores[0][\"rouge-2\"][\"p\"]) \n",
        "  recall.append(scores[0][\"rouge-2\"][\"r\"]) \n",
        "  f1_score.append(scores[0][\"rouge-2\"][\"f\"])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPUF-UI35lQE"
      },
      "source": [
        "precision, recall, f1_score"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puVi1x74FfGh"
      },
      "source": [
        "### **Question 7 (bonus): Finetuning seq2seq model**\n",
        "\n",
        "Exploit the BBC dataset to finetune BART-based model on the proposed dataset. Create a fine-tuning procedure using the article text as input and the ground-truth summary as output of the model.\n",
        "\n",
        "Exploit the [Datasets framework](https://huggingface.co/docs/datasets/) and [Trainer API](https://huggingface.co/transformers/training.html#fine-tuning-in-pytorch-with-the-trainer-api) for training and evaluating the model.\n",
        "\n",
        "Even in this case, evaluate the model using ROUGE-2 precision, recall and f1-score. At this time, you may want to use [metrics python library](https://huggingface.co/metrics) to set the [`compute_metrics`](https://huggingface.co/transformers/main_classes/trainer.html#id1) parameter in Trainer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PELrns0Xo1r9"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}